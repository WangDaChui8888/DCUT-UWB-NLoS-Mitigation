# -*- coding: utf-8 -*-
"""
calculate_complexity.py (v3.4 - ä¿®å¤ NameError)

æ”¹è¿›ç‚¹ï¼š
1) ä¿®å¤äº† logging é…ç½®ä¸­å› ç¼ºå°‘ä¸€è¡Œä»£ç å¯¼è‡´çš„ NameErrorã€‚
2) å†…å­˜æµ‹é‡ç»Ÿä¸€æ”¹ä¸ºæ›´ç¨³å®šçš„â€œé™æ€æ¨¡å‹æ–‡ä»¶å¤§å°â€ã€‚
3) ä¿®å¤äº†å› éƒ¨åˆ†æ¨¡å‹åˆ†æå¤±è´¥å¯¼è‡´ç»“æœé”™ä½çš„é—®é¢˜ã€‚
"""

import sys
from pathlib import Path
import os
import warnings
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from memory_profiler import memory_usage
import logging
from collections import defaultdict
import pickle

# --- å°†è„šæœ¬è‡ªèº«æ‰€åœ¨çš„ç›®å½•æ·»åŠ åˆ° Python æœç´¢è·¯å¾„ ---
SCRIPT_DIR = Path(__file__).resolve().parent
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

# --- ä¾èµ–é¡¹ç›®å†…æ¨¡å— ---
try:
    from model_definition import DualChannelTransformerModel, PositionalEmbedding
except ImportError as e:
    print(f"âŒ é”™è¯¯: æ— æ³•ä» 'model_definition.py' å¯¼å…¥è‡ªå®šä¹‰å±‚æˆ–æ¨¡å‹ã€‚ ({e})")
    sys.exit(1)

try:
    import config
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° config.pyã€‚")
    sys.exit(1)

# --- æ—¥å¿—é…ç½® ---
def setup_logging():
    log_formatter = logging.Formatter("%(asctime)s [%(levelname)-7s]  %(message)s")
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    
    log_file = SCRIPT_DIR / "complexity_analysis_statistical.log"
    file_handler = logging.FileHandler(log_file, mode='w')
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)

    # =========================================================
    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ã€é‡è¦ã€‘æ¢å¤è¢«åˆ é™¤çš„ä»£ç  â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² æ¢å¤ç»“æŸ â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    # =========================================================
    
    logging.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œæ—¥å¿—å°†è®°å½•åˆ°: {log_file}")

# --- ç»“æœç›®å½• ---
BASE_RESULTS_DIR = SCRIPT_DIR / "Ghent_Statistical_Runs"
RUN_DIRS = []
if BASE_RESULTS_DIR.exists():
    RUN_DIRS = sorted([d for d in BASE_RESULTS_DIR.iterdir() if d.is_dir() and d.name.startswith('run_seed_')])

if not RUN_DIRS:
    # å°è¯•å¯»æ‰¾ Ablation_Study_Runs_Advanced ç›®å½•ä½œä¸ºå¤‡é€‰
    BASE_RESULTS_DIR_ALT = SCRIPT_DIR / "Ablation_Study_Runs_Advanced"
    if BASE_RESULTS_DIR_ALT.exists():
        subdirs = [d for d in BASE_RESULTS_DIR_ALT.iterdir() if d.is_dir()]
        if subdirs:
            # è¿›å…¥ç¬¬ä¸€å±‚å­ç›®å½•å¯»æ‰¾ seed æ–‡ä»¶å¤¹
            RUN_DIRS = sorted([d for d in subdirs[0].iterdir() if d.is_dir() and d.name.startswith('seed_')])
            BASE_RESULTS_DIR = subdirs[0] # æ›´æ–°åŸºç¡€ç›®å½•

if not RUN_DIRS:
    logging.error(f"âŒ é”™è¯¯: åœ¨ '{BASE_RESULTS_DIR}' æˆ–å¤‡é€‰ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• 'run_seed_*' æˆ– 'seed_*' æ–‡ä»¶å¤¹ã€‚")
    sys.exit(1)

# --- é™å™ª ---
warnings.filterwarnings("ignore", category=UserWarning, module='tensorflow')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')


# ========================= æ ¸å¿ƒå·¥å…·å‡½æ•° =========================
def get_flops_for_tf_model(model, concrete_input_shapes):
    try:
        input_specs = [tf.TensorSpec(shape, dtype=tf.float32) for shape in concrete_input_shapes]
        @tf.function
        def model_fn(*inputs):
            x = inputs if len(inputs) > 1 else inputs[0]
            return model(x, training=False)
        concrete_func = model_fn.get_concrete_function(*input_specs)
        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
        frozen_func = convert_variables_to_constants_v2(concrete_func)
        graph_def = frozen_func.graph.as_graph_def(add_shapes=True)
        with tf.Graph().as_default() as graph:
            tf.compat.v1.import_graph_def(graph_def, name="")
            run_meta = tf.compat.v1.RunMetadata()
            opts = (tf.compat.v1.profiler.ProfileOptionBuilder(tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())
                    .select(['float_ops']).order_by('float_ops').build())
            flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='op', options=opts)
            return float(flops.total_float_ops if flops else 0)
    except Exception:
        return 0.0

# ========================= åˆ†æå‡½æ•° (è¿”å›åŸå§‹æ•°å€¼, å†…å­˜æ”¹ä¸ºé™æ€å¤§å°) =========================
def analyze_tf_model(run_dir, model_name, model_filename, custom_objects, concrete_input_shapes):
    model_path = run_dir / model_filename
    if not model_path.exists():
        logging.warning(f"  åœ¨ {run_dir.name} ä¸­, æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {model_filename}ï¼Œè·³è¿‡ã€‚")
        return None
    try:
        logging.info(f"--- æ­£åœ¨åˆ†æ TensorFlow æ¨¡å‹: {model_name} ---")
        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)
        params_k = model.count_params() / 1e3
        mflops = get_flops_for_tf_model(model, concrete_input_shapes) / 1e6

        mem_kb = model_path.stat().st_size / 1024
        
        logging.info(f"  åˆ†æå®Œæˆ: Parameters={params_k:.2f}K, FLOPs={mflops:.2f}M, Model Size={mem_kb:.2f}KB")
        return {
            "Parameters (K)": params_k,
            "FLOPs (M)": mflops,
            "Model Size (KB)": mem_kb
        }
    except Exception as e:
        logging.error(f"  åˆ†æ {model_name} ({run_dir.name}) æ—¶å‡ºé”™: {e}", exc_info=False)
        return None

def analyze_sklearn_model(run_dir, model_name):
    logging.info(f"--- æ­£åœ¨åˆ†æ Scikit-learn æ¨¡å‹: {model_name} ---")
    try:
        model_paths = {}
        if model_name == "LS-SVM":
            model_paths = {"cls": run_dir / config.SVC_FINAL_MODEL_FILE, "reg": run_dir / config.RIDGE_FINAL_MODEL_FILE}
        elif model_name == "XGBoost":
            model_paths = {"cls": run_dir / config.XGBOOST_CLS_FINAL_MODEL_FILE, "reg": run_dir / config.XGBOOST_REG_FINAL_MODEL_FILE}
        else: return None
            
        params, mem_kb = 0, 0
        for key, path in model_paths.items():
            if not path.exists():
                logging.warning(f"  åœ¨ {run_dir.name} ä¸­, {model_name} çš„ {path.name} æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡ã€‚")
                return None
            model_obj = joblib.load(path)
            mem_kb += path.stat().st_size / 1024
            
            if model_name == "LS-SVM" and key == "cls":
                params = model_obj.n_support_.sum()
            elif model_name == "XGBoost":
                params += model_obj.get_booster().num_boosted_rounds()
        
        param_metric = "Parameters (SVs)" if model_name == "LS-SVM" else "Parameters (Trees)"
        
        logging.info(f"  åˆ†æå®Œæˆ: {param_metric.split(' ')[0]}={params}, Model Size={mem_kb:.2f}KB")
        return {param_metric: params, "Model Size (KB)": mem_kb}
    except Exception as e:
        logging.error(f"  åˆ†ææ¨¡å‹ {model_name} ({run_dir.name}) æ—¶å‡ºé”™: {e}", exc_info=False)
        return None

# ========================= ä¸»æµç¨‹ =========================
if __name__ == "__main__":
    setup_logging()
    logging.info("=" * 60)
    logging.info("ğŸš€ æ¨¡å‹å¤æ‚åº¦ç»Ÿè®¡åˆ†æè„šæœ¬ v3.4 (ç»ˆæä¿®å¤ç‰ˆ) å¯åŠ¨")
    logging.info(f"   å°†åˆ†æä»¥ä¸‹ {len(RUN_DIRS)} ä¸ªç›®å½•: {[d.name for d in RUN_DIRS]}")
    logging.info("=" * 60)

    all_results = defaultdict(lambda: defaultdict(list))
    custom_objects = {"PositionalEmbedding": PositionalEmbedding, "DualChannelTransformerModel": DualChannelTransformerModel}

    for run_dir in RUN_DIRS:
        logging.info(f"--- æ­£åœ¨å¤„ç†ç›®å½•: {run_dir.name} ---")
        
        num_cir_features = len(config.INITIAL_CIR_FEATURES)
        shape_single = [(1, num_cir_features)]
        flos_dim = 0
        try:
            gmm_los = joblib.load(run_dir / config.GMM_LOS_FILENAME)
            gmm_nlos = joblib.load(run_dir / config.GMM_NLOS_FILENAME)
            flos_dim = int(gmm_los.n_components) + int(gmm_nlos.n_components)
        except Exception:
            logging.warning(f"  åœ¨ {run_dir.name} ä¸­æ‰¾ä¸åˆ°GMMæ–‡ä»¶ï¼Œæ— æ³•åˆ†æ DCUTã€‚")
        
        shape_dual = [(1, num_cir_features), (1, flos_dim)] if flos_dim > 0 else []

        model_analysis_functions = {
            "DCUT": lambda: analyze_tf_model(run_dir, "DCUT", config.DUAL_TRANSFORMER_FINAL_MODEL_FILE, custom_objects, shape_dual),
            "SingleTransformer": lambda: analyze_tf_model(run_dir, "SingleTransformer", config.SINGLE_TRANSFORMER_FINAL_MODEL_FILE, custom_objects, shape_single),
            "DNN": lambda: analyze_tf_model(run_dir, "DNN", config.DNN_FINAL_MODEL_FILE, {}, shape_single),
            "CNN-LSTM": lambda: analyze_tf_model(run_dir, "CNN-LSTM", config.CNN_LSTM_FINAL_MODEL_FILE, {}, shape_single),
            "LS-SVM": lambda: analyze_sklearn_model(run_dir, "LS-SVM"),
            "XGBoost": lambda: analyze_sklearn_model(run_dir, "XGBoost"),
        }
        
        for name, func in model_analysis_functions.items():
            if name == "DCUT" and not shape_dual: continue
            res = func()
            if res:
                for metric, value in res.items():
                    all_results[name][metric].append(value)

    if not all_results:
        logging.warning("âŒ æœªèƒ½æˆåŠŸåˆ†æä»»ä½•æ¨¡å‹ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
    else:
        final_summary = []
        model_order = ["DCUT", "SingleTransformer", "DNN", "CNN-LSTM", "LS-SVM", "XGBoost"]
        
        for model_name in model_order:
            if model_name not in all_results: continue
            
            data = all_results[model_name]
            row = {"Model": model_name}
            
            for metric, values in sorted(data.items()):
                if not values: continue
                mean = np.mean(values)
                std = np.std(values)
                row[metric] = f"{mean:.2f} Â± {std:.2f}" if len(values) > 1 and std > 1e-9 else f"{mean:.2f}"
            final_summary.append(row)

        summary_df = pd.DataFrame(final_summary).set_index("Model")
        
        logging.info("\n" + "=" * 80)
        logging.info("ğŸ“Š æ¨¡å‹å¤æ‚åº¦æœ€ç»ˆæ±‡æ€» (å‡å€¼ Â± æ ‡å‡†å·®)")
        logging.info("=" * 80)
        for line in summary_df.to_string().split('\n'): logging.info(line)
        logging.info("=" * 80)

        csv_path = BASE_RESULTS_DIR / "model_complexity_summary_statistical.csv"
        try:
            summary_df.to_csv(csv_path)
            logging.info(f"âœ… æ±‡æ€»è¡¨æ ¼å·²æˆåŠŸä¿å­˜åˆ°: {csv_path}")
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜ CSV æ–‡ä»¶å¤±è´¥: {e}")

        logging.info("\nè¯´æ˜:")
        logging.info(" - æ‰€æœ‰æ•°å€¼å‡åŸºäºå¯¹å¤šä¸ªéšæœºç§å­è¿è¡Œç»“æœçš„ç»Ÿè®¡åˆ†æã€‚")
        logging.info(" - Parameters (K/SVs/Trees): K=åƒ(1e3), SVs=æ”¯æŒå‘é‡æ•°, Trees=æ ‘çš„æ•°é‡ã€‚")
        logging.info(" - FLOPs (M): M=å…†(1e6)ã€‚è¡¨ç¤ºå•æ¬¡æ¨ç†çš„è®¡ç®—é‡ï¼ˆbatch=1ï¼‰ã€‚")
        logging.info(" - Model Size (KB): æ¨¡å‹åœ¨ç£ç›˜ä¸Šçš„é™æ€å­˜å‚¨å¤§å°ã€‚")

    logging.info("ğŸš€ ç»Ÿè®¡åˆ†æè„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")