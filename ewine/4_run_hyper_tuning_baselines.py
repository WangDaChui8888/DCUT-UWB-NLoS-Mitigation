# -*- coding: utf-8 -*-
"""
Systematic hyper-parameter tuning for baselines (DNN, ST, LS-SVM, XGBoost, CNN-LSTM)
- Unified early stopping protocol (val-BA) and logging.
- Writes a CSV of all trials and prints per-model best config.
"""
import subprocess
from pathlib import Path
import time
import pandas as pd
import itertools
from datetime import datetime
# ===================== åŸºæœ¬è·¯å¾„é…ç½® =====================
MAIN_SCRIPT = "4_main_script_baselines.py"
PROJECT_DIR = Path(__file__).resolve().parent
BASE_SAVE_DIR = PROJECT_DIR / "Tuning_Baselines_R1"
LOG_FILE_PATH = PROJECT_DIR / "tuning_log_baselines.txt"

# ä½ çš„æ±‡æ€»æ–‡ä»¶åï¼ˆä» config ä¸­è¯»å–å¤±è´¥æ—¶çš„å…œåº•ï¼‰
DEFAULT_SUMMARY_CSV = "summary.csv"

# ç»“æœCSVé‡Œâ€œå¾—åˆ†åˆ—â€çš„å€™é€‰åï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
# ä¿®æ”¹å
SCORE_COLUMN_CANDIDATES = [
    "balanced_accuracy_cv_mean",   # <-- æ”¹æˆå°å†™ï¼Œä¸CSVæ–‡ä»¶ä¸­çš„åˆ—åå®Œå…¨ä¸€è‡´
    "val_BA",                      
    "BA_cv_mean",                  
    "BA"                           
]

# ===================== é¡ºåºè°ƒå‚å»ºè®®ï¼ˆå¯é€‰é˜…è¯»ï¼‰ =====================
# ä½ å·²ç»åœ¨ä¸»ä½“è®ºæ–‡å¯¹ DCUT(æˆ‘ä»¬çš„æ¨¡å‹)åšäº†é¡ºåºè°ƒå‚ï¼ˆArchitectureâ†’Learningâ†’Capacityï¼‰
# è¿™é‡Œé’ˆå¯¹â€œåŸºçº¿â€åšä¸€æ¬¡â€œå¹¶åˆ—çš„ç³»ç»ŸåŒ–æœç´¢â€ï¼Œæ¯ä¸ªæ¨¡å‹ç»™å‡ºä¸€ä¸ªæœç´¢ç©ºé—´å’Œå›ºå®šè®­ç»ƒåè®®ã€‚

# ===================== å…¬å…±è®­ç»ƒ/æ—©åœå‚æ•°ï¼ˆæ·±åº¦æ¨¡å‹ï¼‰ =====================
# è‹¥ main_script æ”¯æŒè¿™äº› flagï¼Œåˆ™ä¼šç”Ÿæ•ˆï¼›ä¸æ”¯æŒä¹Ÿæ— å¦¨ï¼ˆå¤šä½™ flag ä¼šè¢«å¿½ç•¥æˆ–éœ€ä½ æ”¹åï¼‰
COMMON_DEEP_ARGS = [
    "--max_epochs", "20",                 # ç»Ÿä¸€è®­ç»ƒé¢„ç®—
    "--batch_size", "128",
    "--optimizer", "adam",
    "--early_stopping", "true",
    "--es_monitor", "val_BA",             # ç›‘æ§éªŒè¯é›† BA
    "--es_patience", "8",
    "--es_min_delta", "1e-4",
]

# ===================== æ¯ä¸ªæ¨¡å‹çš„æœç´¢ç©ºé—´ï¼ˆç³»ç»ŸåŒ–ã€å¯å¤ç°ï¼‰ =====================
# æ³¨æ„ï¼šé”®åè¦ä¸ main_script.py çš„ argparse flag å¯¹é½ï¼›value ç»™å®šå€™é€‰åˆ—è¡¨å³å¯ã€‚
TUNING_GRID = {
    # ---- Single-Transformer baseline (ST) ----
    "SingleTransformer": {
        "--transformer_layers": [2, 3, 4],
        "--attention_heads":    [2, 4, 8],
        "--ff_dim":             [128, 256],
        "--loss_weight_error":  [0.3, 0.6],
        "--learning_rate":      [1e-4, 2e-4],
        "--dropout_rate":       [0.10, 0.20],
    },

    # ---- DNN (MLP) ----
    # å¦‚æœ main_script æ¥æ”¶ --dense_units ä½œä¸ºé€—å·åˆ†éš”çš„å•å±‚å®½åº¦ï¼Œä½ å¯ä»¥ä¼  "128" æˆ– "256"
    # è‹¥æ”¯æŒå¤šå±‚ï¼ˆå¦‚ "128,64"ï¼‰ï¼Œå¯åœ¨åˆ—è¡¨é‡Œç›´æ¥æ”¾å­—ç¬¦ä¸² "128,64"
    "DNN": {
        "--dense_units":   ["64", "128", "256"],   # æˆ– ["128,64", "256,128"] å¦‚æœæ”¯æŒå¤šå±‚
        "--dropout_rate":  [0.0, 0.1, 0.2],
        "--learning_rate": [5e-5, 1e-4, 5e-4],
        "--weight_decay":  [0.0, 1e-5],
        "--activation":    ["relu"],               # å›ºå®šä¸º reluï¼›è‹¥è¦æ¢ç´¢å¯ç»™ ["relu","gelu"]
        "--loss_weight_error": [0.6],              # ä¸ä½ ä¸»ä½“è®¾å®šä¸€è‡´ï¼›å¦‚éœ€æœç´¢å¯å¼€æ”¾ [0.3,0.6]
    },

    # ---- LS-SVM ï¼ˆåˆ†ç±»ï¼‰----
    # ä¼ ç»Ÿæ¨¡å‹é€šå¸¸æ²¡æœ‰ epochsï¼›è¿™é‡Œåªåšæ ¸å‚æ•°ç½‘æ ¼
    # è‹¥ main_script é‡Œçš„æ¨¡å‹åæ˜¯ "SVM" æˆ– "LS_SVM"ï¼ŒæŠŠæ­¤å¤„é”®æ”¹æ‰ï¼Œå¹¶åœ¨ MODEL_NAME_MAP æ˜ å°„
    "LS-SVM": {
        "--svm_kernel": ["rbf"],  # å›ºå®šRBF
        "--svm_C":      [0.1, 0.3, 1, 3, 10],
        "--svm_gamma":  [1e-3, 3e-3, 1e-2, 3e-2, 1e-1],
        "--svm_class_weight": ["balanced"],
        "--standardize": ["true"],   # ç»Ÿä¸€åšæ ‡å‡†åŒ–
    },

    # # ---- XGBoost ï¼ˆåˆ†ç±»/å›å½’åˆ†æ”¯è‹¥åˆå¹¶åœ¨ main_scriptï¼Œä¸­æ€§å‘½åå³å¯ï¼‰----
    "XGBoost": {
        "--xgb_n_estimators":   [100, 300, 600],
        "--xgb_max_depth":      [3, 5, 7],
        "--xgb_learning_rate":  [0.05, 0.1, 0.2],
        "--xgb_subsample":      [0.7, 1.0],
        "--xgb_colsample_bytree":[0.7, 1.0],
        "--xgb_gamma":          [0.0, 1.0],
        "--xgb_min_child_weight":[1, 5],
    },

    # ---- CNN-LSTM ----
    "CNNLSTM": {
        "--conv_filters":  [32, 64],
        "--kernel_size":   [3, 5],
        "--pool_size":     [2],
        "--lstm_units":    [32, 64],
        "--dense_units":   ["64", "128"],
        "--dropout_rate":  [0.0, 0.2],
        "--learning_rate": [1e-4, 5e-4],
        "--activation":    ["relu"],
        "--loss_weight_error": [0.5, 0.6],
    },
}

# è‹¥ --run_only æœŸæœ›çš„å†…éƒ¨æ ‡è¯†ä¸ä½ è¿™é‡Œçš„é”®ä¸åŒï¼Œå¯åœ¨æ­¤æ˜ å°„
MODEL_NAME_MAP = {
    "SingleTransformer": "SingleTransformer",
    "DNN":               "DNN",
    "LS-SVM":            "SVM",           # ä¾‹å¦‚ main_script é‡Œç”¨ "SVM"
    "XGBoost":           "XGBoost",
    "CNNLSTM":          "CNNLSTM",
    # "DualTransformer": "DualTransformer",  # éœ€è¦çš„è¯ä¹Ÿå¯åŠ å…¥
}

# ===================== è°ƒåº¦ä¸è®°å½• =====================
def run_single_tuning_config(model_key, params_tuple, run_id):
    """è¿è¡Œå•æ¬¡è¶…å‚ç»„åˆ"""
    model_for_cli = MODEL_NAME_MAP.get(model_key, model_key)
    save_dir = BASE_SAVE_DIR / model_key / f"run_{run_id}"
    save_dir.mkdir(parents=True, exist_ok=True)

    param_str = " | ".join([f"{k}={v}" for k, v in zip(params_tuple[::2], params_tuple[1::2])])
    print(f"\n--- ğŸš€ Tuning {model_key} (Run {run_id}) | {param_str} ---")

    # ç»„è£…å‘½ä»¤
    command = [
        "python", MAIN_SCRIPT,
        "--save_dir", str(save_dir),
        "--run_only", str(model_for_cli),
    ]

    # æ·±åº¦æ¨¡å‹åŠ å…¬å…±è®­ç»ƒ/æ—©åœå‚æ•°ï¼›ä¼ ç»Ÿæ¨¡å‹ç•¥è¿‡ï¼ˆä¸ä¼šæŠ¥é”™ä¹Ÿè¡Œï¼‰
    if model_key in {"SingleTransformer", "DNN", "CNNLSTM"}:
        command += list(map(str, COMMON_DEEP_ARGS))

    # æ‹¼æ¥æœ¬æ¬¡è¶…å‚
    command += [str(p) for p in params_tuple]

    try:
        subprocess.run(command, check=True, text=True, cwd=PROJECT_DIR)
        print(f"   âœ… SUCCESS: Run {run_id} for {model_key} completed.")
        return save_dir
    except subprocess.CalledProcessError as e:
        print(f"   âŒ FAILED: Run {run_id} for {model_key} failed.")
        print(e.stdout)
        print(e.stderr)
        return None

def _pick_score_column(df):
    for c in SCORE_COLUMN_CANDIDATES:
        if c in df.columns:
            return c
    # æ²¡æœ‰åŒ¹é…åˆ—æ—¶æŠ›é”™ï¼Œæé†’ä½ æ£€æŸ¥ pipeline
    raise KeyError(f"No score column found among: {SCORE_COLUMN_CANDIDATES}. Available: {list(df.columns)}")

def find_best_params(all_results_list):
    """åˆ†ææ‰€æœ‰è°ƒä¼˜ç»“æœï¼Œæ‰¾åˆ°æ¯ä¸ªæ¨¡å‹çš„æœ€ä½³å‚æ•°ï¼Œå¹¶ä¿å­˜å®Œæ•´ç»“æœåˆ°CSVã€‚"""
    print("\n" + "*"*80)
    print("ğŸ† Finding best hyperparameters and saving all results...")

    if not all_results_list:
        print("   [Warning] No results to analyze. Exiting analysis.")
        return

    full_results_df = pd.DataFrame(all_results_list)
    best_params_all_models = {}

    for model_key in TUNING_GRID.keys():
        model_df = full_results_df[full_results_df['Model'] == model_key].copy()
        # å°† Score è½¬ä¸ºæ•°å€¼
        model_df['Score'] = pd.to_numeric(model_df['Score'], errors='coerce').fillna(-1)

        if not model_df.empty and model_df['Score'].max() > -1:
            best_run = model_df.loc[model_df['Score'].idxmax()]
            best_score = float(best_run['Score'])
            # å–è¯¥æ¨¡å‹gridä¸­çš„åˆ—å
            param_cols = list(TUNING_GRID[model_key].keys())
            best_params = {k: best_run.get(k, None) for k in param_cols}

            print(f"\n   ğŸ‰ Best parameters for {model_key}:")
            print(f"      Score (validation BA or cv mean): {best_score:.4f}")
            print(f"      Parameters: {best_params}")
            best_params_all_models[model_key] = best_params
        else:
            print(f"\n   Could not determine best parameters for {model_key}.")

    print("\n" + "*"*80)
    print("Hyperparameter tuning summary:")
    for model, params in best_params_all_models.items():
        print(f"   Model: {model}, Best Params: {params}")

    output_path = BASE_SAVE_DIR / "hyperparameter_tuning_full_results.csv"
    full_results_df.to_csv(output_path, index=False, float_format="%.6f")
    print(f"\nâœ… Full tuning results saved to: {output_path}")

def read_score_from_summary(summary_file, model_key):
    """ä»å•æ¬¡è¿è¡Œçš„ summary.csv é‡Œè¯»å–å¾—åˆ†ï¼ˆå°½é‡é€šç”¨ï¼‰"""
    try:
        df = pd.read_csv(summary_file)
    except Exception as e:
        # å…¼å®¹ index_col="Model" çš„æ—§æ ¼å¼
        try:
            df = pd.read_csv(summary_file, index_col="Model")
            df = df.reset_index()
        except Exception:
            raise e

    score_col = _pick_score_column(df)

    # è‹¥ summary é‡Œæœ‰ Model åˆ—ï¼Œåˆ™æŒ‰æ¨¡å‹åç­›ï¼›å¦åˆ™å–ç¬¬ä¸€è¡Œ
    if "Model" in df.columns:
        # æœ‰çš„ summary ä¼šè®°å½•å†…éƒ¨æ¨¡å‹åï¼ˆå¦‚ "SVM" è€Œä¸æ˜¯ "LS-SVM"ï¼‰ï¼Œåšä¸€æ¬¡æ˜ å°„å…¼å®¹
        internal_name = MODEL_NAME_MAP.get(model_key, model_key)
        sub = df[df["Model"].astype(str).str.lower() == str(internal_name).lower()]
        if not sub.empty:
            return float(sub.iloc[0][score_col])
        # å…œåº•ï¼šå–å…¨è¡¨æœ€å¤§
        return float(df[score_col].max())
    else:
        return float(df.iloc[0][score_col])

# ===================== ä¸»æµç¨‹ =====================
if __name__ == "__main__":
    script_start_time = time.time()

    BASE_SAVE_DIR.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE_PATH, "w", encoding="utf-8") as f:
        f.write(f"--- Hyperparameter Tuning Log (Baselines) | Started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n\n")

    all_run_results = []

    for model_key, grid in TUNING_GRID.items():
        param_names = list(grid.keys())
        param_values = list(grid.values())
        param_combinations = list(itertools.product(*param_values))

        print(f"\nFound {len(param_combinations)} combinations for {model_key}.")
        run_id_counter = 1

        for combo in param_combinations:
            params_tuple = tuple(itertools.chain(*zip(param_names, combo)))

            run_start_time = time.time()
            save_dir = run_single_tuning_config(model_key, params_tuple, run_id_counter)
            duration_seconds = time.time() - run_start_time
            print(f"   â° Time for this run: {duration_seconds:.2f} seconds ({duration_seconds/60:.2f} minutes).")

            status = "FAILED"
            score = "N/A"
            stopped_epoch = "N/A"
            current_params = {name: val for name, val in zip(param_names, combo)}

            if save_dir:
                # è¯»å– summary
                try:
                    # ä¼˜å…ˆè¯» config é‡Œå®šä¹‰çš„åç§°
                    try:
                        from config import SUMMARY_CSV_FILENAME as CFG_SUMMARY
                    except Exception:
                        CFG_SUMMARY = DEFAULT_SUMMARY_CSV
                    summary_file = save_dir / CFG_SUMMARY
                    if summary_file.exists():
                        try:
                            score_val = read_score_from_summary(summary_file, model_key)
                            score = f"{score_val:.6f}"
                            status = "SUCCESS"
                            # Optional: è‹¥ summary æœ‰æ—©åœè½®æ•°åˆ—ï¼Œå¯ä¸€å¹¶è¯»å–
                            try:
                                df_tmp = pd.read_csv(summary_file)
                                for candidate in ["stopped_epoch", "stopped_epoch_cv_mean", "early_stop_epoch"]:
                                    if candidate in df_tmp.columns:
                                        stopped_epoch = df_tmp[candidate].iloc[0]
                                        break
                            except Exception:
                                pass
                        except Exception as e:
                            status = f"ERROR_READING_CSV: {e}"
                    else:
                        status = "NO_SUMMARY_FILE"
                except Exception as e:
                    status = f"EXC: {e}"

            # è®°å½•å•æ¬¡
            result_data = {
                "Model": model_key,
                "RunID": run_id_counter,
                "Status": status,
                "Score": score,
                "StoppedEpoch": stopped_epoch,
                **current_params,
                "Duration_sec": round(duration_seconds, 2),
            }
            all_run_results.append(result_data)

            # è§„èŒƒåŒ–æ—©åœæ˜¾ç¤º
            stopped_epoch_str = f"{stopped_epoch:.1f}" if isinstance(stopped_epoch, (int, float)) else str(stopped_epoch)
            log_entry = (
                f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] "
                f"Model={model_key}, Run={run_id_counter}, Status={status}, "
                f"Duration={duration_seconds:.2f}s, Score={score}, StoppedEpoch={stopped_epoch_str}, "
                f"Params={current_params}\n"
            )
            with open(LOG_FILE_PATH, "a", encoding="utf-8") as f:
                f.write(log_entry)

            run_id_counter += 1

    # æ±‡æ€»ä¸æœ€ä½³
    find_best_params(all_run_results)

    total_duration_minutes = (time.time() - script_start_time) / 60
    print(f"\nâœ¨ Hyperparameter tuning finished. Total time: {total_duration_minutes:.2f} minutes.")
